{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8860961",
   "metadata": {},
   "source": [
    "# EcoHome Energy Advisor - Agent Run & Evaluation\n",
    "\n",
    "In this notebook, we'll run the Energy Advisor agent with various real-world scenarios and see how it helps customers optimize their energy usage.\n",
    "\n",
    "- Create the agent's instructions\n",
    "- Run the Energy Advisor with different types of questions\n",
    "- Evaluate response quality and accuracy\n",
    "- Measure tool usage effectiveness\n",
    "- Identify areas for improvement\n",
    "- Implement evaluation metrics\n",
    "\n",
    "## Evaluation Criteria\n",
    "- **Accuracy**: Correct information and calculations\n",
    "- **Relevance**: Responses address the user's question\n",
    "- **Completeness**: Comprehensive answers with actionable advice\n",
    "- **Tool Usage**: Appropriate use of available tools\n",
    "- **Reasoning**: Clear explanation of recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ffc7d",
   "metadata": {},
   "source": [
    "## 1. Import and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b989c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from agent import Agent\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, Any, List, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d063734",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the agent's instructions\n",
    "\n",
    "ECOHOME_SYSTEM_PROMPT = \"\"\"\n",
    "You are the EcoHome Energy Advisor, an AI assistant that helps homeowners optimize\n",
    "their energy usage, electricity costs, and environmental impact.\n",
    "\n",
    "You have access to these tools:\n",
    "- get_weather_forecast(location, days): weather forecasts with solar-relevant info.\n",
    "- get_electricity_prices(date): hourly time-of-use (TOU) electricity prices.\n",
    "- query_energy_usage(start_date, end_date, device_type): historical household usage.\n",
    "- query_solar_generation(start_date, end_date): historical solar generation.\n",
    "- get_recent_energy_summary(hours): recent usage + solar overview by device and cost.\n",
    "- search_energy_tips(query): retrieval-augmented energy-saving tips and best practices.\n",
    "- calculate_energy_savings(device_type, current_usage_kwh, optimized_usage_kwh, price_per_kwh):\n",
    "  estimate savings in kWh, dollars, and annual impact.\n",
    "\n",
    "YOUR KEY CAPABILITIES\n",
    "\n",
    "Weather Integration (solar-aware optimization):\n",
    "- Use get_weather_forecast to understand cloud cover, temperature, and sunlight.\n",
    "- Use this to:\n",
    "  - Identify when solar production will be highest.\n",
    "  - Suggest running flexible loads (EV charging, pool pumps, appliances) during sunny hours.\n",
    "  - Recommend pre-cooling or pre-heating before extreme temperature periods.\n",
    "\n",
    "Dynamic Pricing (time-of-use optimization):\n",
    "- Use get_electricity_prices to:\n",
    "  - Identify off-peak, mid-peak, and on-peak hours.\n",
    "  - Shift flexible device usage from high-cost to low-cost periods.\n",
    "- Always consider both price and solar availability when recommending schedules.\n",
    "\n",
    "Historical Analysis (personalized advice):\n",
    "- Use query_energy_usage and/or get_recent_energy_summary to:\n",
    "  - Identify which devices consume the most energy and cost.\n",
    "  - Detect patterns (e.g., EV charging at expensive times, HVAC dominating usage).\n",
    "- Use query_solar_generation to understand how much solar is typically available.\n",
    "- Base your suggestions on this history whenever the user asks about \"my usage\"\n",
    "  or \"based on my data/history.\"\n",
    "\n",
    "RAG Pipeline (energy-saving knowledge):\n",
    "- Use search_energy_tips when the user asks for:\n",
    "  - “tips”, “ways to reduce energy”, “best practices”, “how can I save more”.\n",
    "- Combine retrieved tips with the user’s historical data and context.\n",
    "- Summarize tips in your own words and keep them specific and actionable.\n",
    "\n",
    "Multi-device Optimization:\n",
    "- Be able to reason about:\n",
    "  - Electric vehicles (EVs) and when to charge them.\n",
    "  - HVAC / thermostat settings and pre-cooling/pre-heating.\n",
    "  - Dishwashers, washing machines, dryers, and other appliances.\n",
    "  - Pool pumps and circulation schedules.\n",
    "  - Solar systems and, if mentioned, energy storage (batteries).\n",
    "- When appropriate, propose a coordinated schedule across multiple devices.\n",
    "\n",
    "Cost Calculations & ROI:\n",
    "- Use calculate_energy_savings to quantify:\n",
    "  - kWh savings.\n",
    "  - Dollar savings per day, month, or year.\n",
    "- Clearly state your assumptions (e.g., kWh per cycle, hours of runtime, price per kWh).\n",
    "- Whenever the user asks “how much can I save” or “what is the impact”, include numbers.\n",
    "\n",
    "BEHAVIOR GUIDELINES\n",
    "\n",
    "- Accuracy:\n",
    "  - Call tools instead of guessing.\n",
    "  - Use the most relevant tools for the user’s question (weather, prices, history, tips, savings).\n",
    "  - Double-check that units (kWh, °C/°F, $, hours) are consistent and reasonable.\n",
    "\n",
    "- Relevance:\n",
    "  - Directly answer the user’s question first.\n",
    "  - Then briefly expand with useful related recommendations (but don’t ramble).\n",
    "\n",
    "- Completeness:\n",
    "  - Provide actionable steps: specific hours, temperatures, or device schedules.\n",
    "  - When possible, give at least 2–3 concrete recommendations instead of just one.\n",
    "\n",
    "- Tool Usage:\n",
    "  - For scheduling questions (\"when should I run/charge…\"), combine:\n",
    "    - get_weather_forecast + get_electricity_prices, and optionally usage history.\n",
    "  - For “based on my usage” questions, use:\n",
    "    - query_energy_usage or get_recent_energy_summary.\n",
    "  - For generic savings tips, use:\n",
    "    - search_energy_tips and then personalize.\n",
    "  - For numeric savings, use:\n",
    "    - calculate_energy_savings.\n",
    "\n",
    "- Reasoning:\n",
    "  - Briefly explain WHY you are making a recommendation\n",
    "    (e.g., “these hours are off-peak and have strong solar production”).\n",
    "  - Keep explanations clear and non-technical.\n",
    "\n",
    "EXAMPLE EXPECTATIONS\n",
    "\n",
    "“When should I charge my electric car tomorrow to minimize cost and maximize solar power?”:\n",
    "  - Use get_weather_forecast and get_electricity_prices (for tomorrow).\n",
    "     Recommend specific charging hours that are both sunny and off-peak.\n",
    "\n",
    "“What temperature should I set my thermostat on Wednesday afternoon if electricity prices spike?”:\n",
    "  - Use get_electricity_prices (for that date) and optionally weather.\n",
    "     Recommend a concrete thermostat range and pre-cooling strategy.\n",
    "\n",
    "“Suggest three ways I can reduce energy use based on my usage history.”:\n",
    "  - Use get_recent_energy_summary or query_energy_usage plus search_energy_tips.\n",
    "     Provide 3 personalized, high-impact actions.\n",
    "\n",
    "“How much can I save by running my dishwasher during off-peak hours?”:\n",
    "  - Use get_electricity_prices and calculate_energy_savings with a reasonable\n",
    "     kWh-per-cycle assumption. Return approximate $ savings per cycle and per month.\n",
    "\n",
    "“What’s the best time to run my pool pump this week based on the weather forecast?”:\n",
    "  - Use get_weather_forecast (and pricing if useful) to choose daily windows that\n",
    "     balance good circulation/solar with acceptable electricity cost.\n",
    "\n",
    "If data or tools are unavailable or return errors, say so briefly and provide\n",
    "the best-practice advice you can, clearly noting that it is based on general\n",
    "guidelines rather than the user’s specific data.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3091beec-0cc4-4df8-ab97-0aa291e44f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aaa54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecohome_agent = Agent(\n",
    "    instructions=ECOHOME_SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c430435-1311-46cf-9bad-57974b1ead98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get_weather_forecast',\n",
       " 'get_electricity_prices',\n",
       " 'query_energy_usage',\n",
       " 'query_solar_generation',\n",
       " 'get_recent_energy_summary',\n",
       " 'search_energy_tips',\n",
       " 'calculate_energy_savings']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecohome_agent.get_agent_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a63a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ecohome_agent.invoke(\n",
    "    question=\"When should I charge my electric car tomorrow to minimize cost and maximize solar power?\",\n",
    "    context=\"Location: San Francisco, CA\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd70a5ba-8e50-4cc2-ba9e-53ee0c453a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To minimize costs and maximize solar power when charging your electric car tomorrow (October 24, 2023), here are the best times to charge based on the weather forecast and electricity prices:\n",
      "\n",
      "### Best Charging Times:\n",
      "1. **Off-Peak Hours (Lowest Cost)**:\n",
      "   - **12:00 AM - 5:00 AM**: $0.132 per kWh\n",
      "   - **11:00 PM - 12:00 AM**: $0.132 per kWh\n",
      "\n",
      "2. **Mid-Peak Hours (Moderate Cost)**:\n",
      "   - **6:00 AM - 10:00 AM**: $0.22 per kWh\n",
      "   - **10:00 PM - 11:00 PM**: $0.242 per kWh\n",
      "\n",
      "### Solar Power Availability:\n",
      "- **Solar Generation**: The forecast indicates that solar generation will start to pick up around **9:00 AM** and peak around **3:00 PM**. However, charging during this time will be more expensive due to on-peak rates.\n",
      "\n",
      "### Recommendations:\n",
      "- **Best Option**: Charge your electric car during the **off-peak hours** from **12:00 AM to 5:00 AM**. This will ensure you are using electricity at the lowest rate.\n",
      "- If you prefer to charge during the day when solar power is available, consider charging from **9:00 AM to 10:00 AM** (though it will be at a mid-peak rate of $0.22 per kWh).\n",
      "\n",
      "By following this schedule, you can take advantage of lower electricity costs while also benefiting from solar energy during the day.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d0aa3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOLS:\n",
      "- get_weather_forecast\n",
      "- get_electricity_prices\n"
     ]
    }
   ],
   "source": [
    "print(\"TOOLS:\")\n",
    "for msg in response[\"messages\"]:\n",
    "    obj = msg.model_dump()\n",
    "    if obj.get(\"tool_call_id\"):\n",
    "        print(\"-\", msg.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d2730e",
   "metadata": {},
   "source": [
    "## 2. Define Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aefe0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive test cases for the Energy Advisor\n",
    "# Create 10 test cases covering different scenarios:\n",
    "# - EV charging optimization\n",
    "# - Thermostat settings\n",
    "# - Appliance scheduling\n",
    "# - Solar power maximization\n",
    "# - Cost savings calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f086892",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"id\": \"ev_charging_1\",\n",
    "        \"question\": \"When should I charge my electric car tomorrow to minimize cost and maximize solar power?\",\n",
    "        \"expected_tools\": [\"get_weather_forecast\", \"get_electricity_prices\"],\n",
    "        \"expected_response\": \"Must recommend specific charging hours, reference sunny periods and off-peak pricing.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"thermostat_2\",\n",
    "        \"question\": \"What temperature should I set my thermostat on Wednesday afternoon to save money?\",\n",
    "        \"expected_tools\": [\"get_electricity_prices\", \"get_weather_forecast\"],\n",
    "        \"expected_response\": \"Should suggest a numeric thermostat range and explain price/weather reasoning.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"dishwasher_3\",\n",
    "        \"question\": \"How much can I save by running my dishwasher at night instead of 6 PM?\",\n",
    "        \"expected_tools\": [\"get_electricity_prices\", \"calculate_energy_savings\"],\n",
    "        \"expected_response\": \"Must estimate savings per cycle and per month using TOU rate difference.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"laundry_4\",\n",
    "        \"question\": \"When is the best time to run my washing machine this weekend?\",\n",
    "        \"expected_tools\": [\"get_electricity_prices\"],\n",
    "        \"expected_response\": \"Should detect weekend pricing and recommend cheapest hours.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"solar_forecast_5\",\n",
    "        \"question\": \"How much solar energy can I expect tomorrow in San Francisco?\",\n",
    "        \"expected_tools\": [\"get_weather_forecast\"],\n",
    "        \"expected_response\": \"Should reference sunny hours, irradiance levels, or expected generation patterns.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"usage_history_6\",\n",
    "        \"question\": \"Which appliance used the most electricity last month?\",\n",
    "        \"expected_tools\": [\"query_energy_usage\"],\n",
    "        \"expected_response\": \"Must identify highest-consumption device and provide kWh + cost breakdown.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"optimization_multi_device_7\",\n",
    "        \"question\": \"Help me schedule my EV, dishwasher, and dryer tomorrow for lowest electricity cost.\",\n",
    "        \"expected_tools\": [\"get_electricity_prices\", \"get_weather_forecast\"],\n",
    "        \"expected_response\": \"Should propose a coordinated time schedule minimizing on-peak usage.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"energy_tips_8\",\n",
    "        \"question\": \"Give me three ways to reduce electricity usage at home.\",\n",
    "        \"expected_tools\": [\"search_energy_tips\"],\n",
    "        \"expected_response\": \"Should return 3 actionable, personalized efficiency recommendations.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"recent_summary_9\",\n",
    "        \"question\": \"Summarize my energy usage over the past 48 hours.\",\n",
    "        \"expected_tools\": [\"get_recent_energy_summary\"],\n",
    "        \"expected_response\": \"Should return total kWh, cost, device breakdown, and insights.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"pool_pump_10\",\n",
    "        \"question\": \"What is the best time to run my pool pump this week?\",\n",
    "        \"expected_tools\": [\"get_weather_forecast\", \"get_electricity_prices\"],\n",
    "        \"expected_response\": \"Must recommend a daily schedule balancing sunlight and off-peak pricing.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "if len(test_cases) < 10:\n",
    "    raise ValueError(\"You MUST have at least 10 test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdde81",
   "metadata": {},
   "source": [
    "## 3. Run Agent Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8eb83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"Location: San Francisco, CA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9dc82e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running Agent Tests ===\n",
      "\n",
      "Test 1: ev_charging_1\n",
      "Question: When should I charge my electric car tomorrow to minimize cost and maximize solar power?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 2: thermostat_2\n",
      "Question: What temperature should I set my thermostat on Wednesday afternoon to save money?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 3: dishwasher_3\n",
      "Question: How much can I save by running my dishwasher at night instead of 6 PM?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 4: laundry_4\n",
      "Question: When is the best time to run my washing machine this weekend?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 5: solar_forecast_5\n",
      "Question: How much solar energy can I expect tomorrow in San Francisco?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 6: usage_history_6\n",
      "Question: Which appliance used the most electricity last month?\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 7: optimization_multi_device_7\n",
      "Question: Help me schedule my EV, dishwasher, and dryer tomorrow for lowest electricity cost.\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 8: energy_tips_8\n",
      "Question: Give me three ways to reduce electricity usage at home.\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 9: recent_summary_9\n",
      "Question: Summarize my energy usage over the past 48 hours.\n",
      "--------------------------------------------------\n",
      "\n",
      "Test 10: pool_pump_10\n",
      "Question: What is the best time to run my pool pump this week?\n",
      "--------------------------------------------------\n",
      "\n",
      "Completed 10 tests\n"
     ]
    }
   ],
   "source": [
    "# Run the agent tests\n",
    "# For each test case, call the agent and collect the response\n",
    "# Store results for evaluation\n",
    "\n",
    "print(\"=== Running Agent Tests ===\")\n",
    "test_results = []\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    print(f\"\\nTest {i+1}: {test_case['id']}\")\n",
    "    print(f\"Question: {test_case['question']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Call the agent\n",
    "        response = ecohome_agent.invoke(\n",
    "            question=test_case['question'],\n",
    "            context=CONTEXT\n",
    "        )\n",
    "        \n",
    "        # Store the result\n",
    "        result = {\n",
    "            'test_id': test_case['id'],\n",
    "            'question': test_case['question'],\n",
    "            'response': response,\n",
    "            'messages': response['messages'] if isinstance(response, dict) and 'messages' in response else [],\n",
    "            'expected_tools': test_case['expected_tools'],\n",
    "            'expected_response': test_case['expected_response'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'failed': False,\n",
    "        }\n",
    "        test_results.append(result)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        result = {\n",
    "            'test_id': test_case['id'],\n",
    "            'question': test_case['question'],\n",
    "            'response': f\"Error: {str(e)}\",\n",
    "            'expected_tools': test_case['expected_tools'],\n",
    "            'expected_response': test_case['expected_response'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'error': str(e)\n",
    "        }\n",
    "        test_results.append(result)\n",
    "\n",
    "print(f\"\\nCompleted {len(test_results)} tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5763a2c-4fd2-41e9-b610-fabb5e8843c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760814c",
   "metadata": {},
   "source": [
    "## 4. Evaluate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement evaluation functions\n",
    "# Create functions to evaluate:\n",
    "# - Final Response\n",
    "# - Tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8845f3be-e5bb-4e51-9225-a9d158ccf47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(text: str) -> str:\n",
    "    \"\"\"Lowercase + remove extra spaces and punctuation for simple matching.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s:.%-]\", \" \", text)  # keep simple symbols like % and :\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0ba37a2-1411-41f9-90e1-e28184867d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(text: str) -> List[str]:\n",
    "    return [t for t in _normalize(text).split(\" \") if t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c1284f1-8482-4329-8052-d20d1a4ede26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _overlap_score(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple token overlap between two strings.\n",
    "    Returns a score between 0 and 1.\n",
    "    \"\"\"\n",
    "    ta = set(_tokenize(a))\n",
    "    tb = set(_tokenize(b))\n",
    "    if not ta or not tb:\n",
    "        return 0.0\n",
    "    inter = len(ta.intersection(tb))\n",
    "    union = len(ta.union(tb))\n",
    "    return inter / union if union > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "031e858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(\n",
    "    question: str,\n",
    "    final_response: str,\n",
    "    expected_response: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a single response against an expected description.\n",
    "\n",
    "    Metrics (0.0 – 1.0):\n",
    "      - ACCURACY: How well the response matches the expected_response content.\n",
    "      - RELEVANCE: How well the response stays on topic with the question.\n",
    "      - COMPLETENESS: Does it cover the key aspects implied in expected_response?\n",
    "      - USEFULNESS: Practical, actionable quality of the answer.\n",
    "\n",
    "    Returns a dict with per-metric scores + feedback and an overall summary.\n",
    "    \"\"\"\n",
    "    if not isinstance(final_response, str):\n",
    "        final_response = str(final_response or \"\")\n",
    "    if not isinstance(expected_response, str):\n",
    "        expected_response = str(expected_response or \"\")\n",
    "    if not isinstance(question, str):\n",
    "        question = str(question or \"\")\n",
    "\n",
    "    resp_norm = _normalize(final_response)\n",
    "    q_norm = _normalize(question)\n",
    "    exp_norm = _normalize(expected_response)\n",
    "\n",
    "    # --- 1. ACCURACY ---\n",
    "    accuracy_raw = _overlap_score(final_response, expected_response)\n",
    "\n",
    "    if accuracy_raw >= 0.75:\n",
    "        acc_feedback = (\n",
    "            \"The response closely aligns with the expected behavior and content.\"\n",
    "        )\n",
    "    elif accuracy_raw >= 0.4:\n",
    "        acc_feedback = (\n",
    "            \"The response partially aligns with expectations but misses some details.\"\n",
    "        )\n",
    "    else:\n",
    "        acc_feedback = (\n",
    "            \"The response diverges significantly from the expected guidance or omits \"\n",
    "            \"important elements.\"\n",
    "        )\n",
    "\n",
    "    # --- 2. RELEVANCE ---\n",
    "    relevance_raw = _overlap_score(final_response, question)\n",
    "\n",
    "    # Penalize clearly generic error responses\n",
    "    if any(x in resp_norm for x in [\"internal error\", \"try again\", \"cannot\", \"sorry\"]):\n",
    "        relevance_raw *= 0.5\n",
    "\n",
    "    if relevance_raw >= 0.75:\n",
    "        rel_feedback = \"The response is highly relevant to the user’s question.\"\n",
    "    elif relevance_raw >= 0.4:\n",
    "        rel_feedback = (\n",
    "            \"The response is somewhat relevant but includes tangential or generic content.\"\n",
    "        )\n",
    "    else:\n",
    "        rel_feedback = \"The response does not adequately address the user’s question.\"\n",
    "\n",
    "    # --- 3. COMPLETENESS ---\n",
    "    completeness_overlap = _overlap_score(final_response, expected_response)\n",
    "    resp_len = len(resp_norm.split())\n",
    "\n",
    "    # Very short answers can’t be fully complete\n",
    "    length_factor = 1.0\n",
    "    if resp_len < 40:\n",
    "        length_factor = 0.4\n",
    "    elif resp_len < 80:\n",
    "        length_factor = 0.7\n",
    "\n",
    "    completeness_raw = min(1.0, completeness_overlap * 1.2 * length_factor)\n",
    "\n",
    "    if completeness_raw >= 0.75:\n",
    "        comp_feedback = (\n",
    "            \"The response is comprehensive and covers the key elements implied by the expectations.\"\n",
    "        )\n",
    "    elif completeness_raw >= 0.4:\n",
    "        comp_feedback = (\n",
    "            \"The response is partially complete; it covers some important aspects but \"\n",
    "            \"misses others.\"\n",
    "        )\n",
    "    else:\n",
    "        comp_feedback = (\n",
    "            \"The response feels incomplete and lacks several important details or steps.\"\n",
    "        )\n",
    "\n",
    "    # --- 4. USEFULNESS ---\n",
    "    has_numbers = bool(re.search(r\"\\d\", resp_norm))\n",
    "    has_actions = any(\n",
    "        kw in resp_norm\n",
    "        for kw in [\n",
    "            \"recommend\",\n",
    "            \"should\",\n",
    "            \"you can\",\n",
    "            \"best time\",\n",
    "            \"set your\",\n",
    "            \"run your\",\n",
    "            \"schedule\",\n",
    "            \"try\",\n",
    "            \"consider\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    base_usefulness = (accuracy_raw + relevance_raw + completeness_raw) / 3.0\n",
    "    action_boost = 0.1 if has_numbers and has_actions else 0.0\n",
    "    usefulness_raw = min(1.0, base_usefulness + action_boost)\n",
    "\n",
    "    if usefulness_raw >= 0.75:\n",
    "        use_feedback = (\n",
    "            \"The response is highly useful, with clear, actionable recommendations.\"\n",
    "        )\n",
    "    elif usefulness_raw >= 0.4:\n",
    "        use_feedback = (\n",
    "            \"The response is somewhat useful but could be more concrete or actionable.\"\n",
    "        )\n",
    "    else:\n",
    "        use_feedback = (\n",
    "            \"The response offers limited practical value and needs clearer, more \"\n",
    "            \"actionable guidance.\"\n",
    "        )\n",
    "\n",
    "    # ---Overall---\n",
    "    overall = (accuracy_raw + relevance_raw + completeness_raw + usefulness_raw) / 4.0\n",
    "\n",
    "    if overall >= 0.8:\n",
    "        overall_feedback = (\n",
    "            \"Overall, this is a strong response: it is accurate, relevant, and actionable \"\n",
    "            \"for the user’s energy optimization question.\"\n",
    "        )\n",
    "    elif overall >= 0.5:\n",
    "        overall_feedback = (\n",
    "            \"Overall, the response is decent but has room for improvement in either \"\n",
    "            \"accuracy, completeness, or practical usefulness.\"\n",
    "        )\n",
    "    else:\n",
    "        overall_feedback = (\n",
    "            \"Overall, the response does not sufficiently meet the expectations. It should \"\n",
    "            \"be more accurate, better aligned with the question, and more concrete.\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": {\n",
    "            \"score\": round(accuracy_raw, 3),\n",
    "            \"feedback\": acc_feedback,\n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"score\": round(relevance_raw, 3),\n",
    "            \"feedback\": rel_feedback,\n",
    "        },\n",
    "        \"completeness\": {\n",
    "            \"score\": round(completeness_raw, 3),\n",
    "            \"feedback\": comp_feedback,\n",
    "        },\n",
    "        \"usefulness\": {\n",
    "            \"score\": round(usefulness_raw, 3),\n",
    "            \"feedback\": use_feedback,\n",
    "        },\n",
    "        \"overall_score\": round(overall, 3),\n",
    "        \"overall_feedback\": overall_feedback,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a3b69ad-28a7-4c4b-94d0-99c922b43b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_tools_used(messages) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract tool names used from a LangGraph / LangChain messages list.\n",
    "    Works with message objects or dicts.\n",
    "    \"\"\"\n",
    "    if messages is None:\n",
    "        return []\n",
    "\n",
    "    used: List[str] = []\n",
    "\n",
    "    for msg in messages:\n",
    "        obj = msg\n",
    "        if not isinstance(obj, dict) and hasattr(msg, \"model_dump\"):\n",
    "            obj = msg.model_dump()\n",
    "\n",
    "        # Tool calls are embedded in AIMessage\n",
    "        tool_calls = obj.get(\"tool_calls\") if isinstance(obj, dict) else None\n",
    "        if tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                name = tc.get(\"name\")\n",
    "                if name:\n",
    "                    used.append(name)\n",
    "\n",
    "        # ToolMessage rows\n",
    "        #   In dict form: type == \"tool\", has name\n",
    "        if isinstance(obj, dict) and obj.get(\"type\") == \"tool\" and obj.get(\"name\"):\n",
    "            used.append(obj[\"name\"])\n",
    "\n",
    "        # Fallback: if it's a ToolMessage-like object\n",
    "        if not isinstance(obj, dict):\n",
    "            if getattr(msg, \"tool_call_id\", None) and getattr(msg, \"name\", None):\n",
    "                used.append(msg.name)\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen: Set[str] = set()\n",
    "    unique_used: List[str] = []\n",
    "    for t in used:\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            unique_used.append(t)\n",
    "\n",
    "    return unique_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e0dc154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tool_usage(messages, expected_tools: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate tool usage based on messages_list and expected_tools.\n",
    "\n",
    "    Metrics:\n",
    "      - Tool Appropriateness: Were the tools used are the right ones?\n",
    "      - Tool Completeness: Were all necessary tools (expected_tools) used?\n",
    "\n",
    "    Returns metrics + detailed feedback.\n",
    "    \"\"\"\n",
    "    expected_set = set(expected_tools or [])\n",
    "    used_tools = _extract_tools_used(messages)\n",
    "    used_set = set(used_tools)\n",
    "\n",
    "    correct_used = used_set.intersection(expected_set)\n",
    "    extra_used = used_set.difference(expected_set)\n",
    "    missing = expected_set.difference(used_set)\n",
    "\n",
    "    # Appropriateness: of the tools the agent did call, how many were expected?\n",
    "    if used_tools:\n",
    "        appropriateness_score = len(correct_used) / len(used_set)\n",
    "    else:\n",
    "        # No tools used at all\n",
    "        appropriateness_score = 1.0 if not expected_set else 0.0\n",
    "\n",
    "    # Completeness: of the tools we expected, how many did the agent use?\n",
    "    if expected_set:\n",
    "        completeness_score = len(correct_used) / len(expected_set)\n",
    "    else:\n",
    "        # No tools required for this test\n",
    "        completeness_score = 1.0\n",
    "\n",
    "    # --- Feedback for Appropriateness ---\n",
    "    if appropriateness_score >= 0.9:\n",
    "        app_feedback = (\n",
    "            \"The agent selected the appropriate tools for this scenario. \"\n",
    "            f\"Used: {sorted(list(used_set)) or ['<none>']}.\"\n",
    "        )\n",
    "    elif appropriateness_score >= 0.5:\n",
    "        app_feedback = (\n",
    "            \"The agent used some appropriate tools, but also included unnecessary \"\n",
    "            f\"or less relevant tools. Extra tools: {sorted(list(extra_used)) or ['<none>']}.\"\n",
    "        )\n",
    "    else:\n",
    "        app_feedback = (\n",
    "            \"The agent often chose inappropriate tools or failed to rely on the right ones. \"\n",
    "            f\"Expected: {sorted(list(expected_set)) or ['<none>']}, \"\n",
    "            f\"but used: {sorted(list(used_set)) or ['<none>']}.\"\n",
    "        )\n",
    "\n",
    "    # --- Feedback for Completeness ---\n",
    "    if completeness_score >= 0.9:\n",
    "        comp_feedback_tool = (\n",
    "            \"The agent called all of the necessary tools for this task.\"\n",
    "        )\n",
    "    elif completeness_score >= 0.5:\n",
    "        comp_feedback_tool = (\n",
    "            \"The agent used some, but not all, of the necessary tools. \"\n",
    "            f\"Missing: {sorted(list(missing)) or ['<none>']}.\"\n",
    "        )\n",
    "    else:\n",
    "        comp_feedback_tool = (\n",
    "            \"The agent missed most of the key tools needed for this scenario. \"\n",
    "            f\"Expected: {sorted(list(expected_set)) or ['<none>']}, \"\n",
    "            f\"Missing: {sorted(list(missing)) or ['<none>']}.\"\n",
    "        )\n",
    "\n",
    "    overall_tool_score = (appropriateness_score + completeness_score) / 2.0\n",
    "    if overall_tool_score >= 0.8:\n",
    "        overall_tool_feedback = (\n",
    "            \"Overall, tool usage is strong: the agent generally picks the right tools \"\n",
    "            \"and uses all that are needed.\"\n",
    "        )\n",
    "    elif overall_tool_score >= 0.5:\n",
    "        overall_tool_feedback = (\n",
    "            \"Overall, tool usage is acceptable but could be more consistent in picking \"\n",
    "            \"all the right tools and avoiding unnecessary calls.\"\n",
    "        )\n",
    "    else:\n",
    "        overall_tool_feedback = (\n",
    "            \"Overall, tool usage needs significant improvement. The agent should rely \"\n",
    "            \"more on the appropriate tools and ensure all required tools are invoked.\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"used_tools\": used_tools,\n",
    "        \"expected_tools\": list(expected_set),\n",
    "        \"tool_appropriateness\": {\n",
    "            \"score\": round(appropriateness_score, 3),\n",
    "            \"feedback\": app_feedback,\n",
    "        },\n",
    "        \"tool_completeness\": {\n",
    "            \"score\": round(completeness_score, 3),\n",
    "            \"feedback\": comp_feedback_tool,\n",
    "        },\n",
    "        \"overall_score\": round(overall_tool_score, 3),\n",
    "        \"overall_feedback\": overall_tool_feedback,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c3df271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report(test_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive evaluation report from test_results.\n",
    "\n",
    "    Each item in test_results is expected to have:\n",
    "      - test_id\n",
    "      - question\n",
    "      - response (raw agent output or error string)\n",
    "      - expected_response (text description of desired behavior)\n",
    "      - expected_tools (list of tool names)\n",
    "      - messages (optional LangChain/LangGraph messages list)\n",
    "      - failed (optional bool)\n",
    "    \"\"\"\n",
    "    if not test_results:\n",
    "        return {\n",
    "            \"summary\": {\n",
    "                \"num_tests\": 0,\n",
    "                \"avg_accuracy\": 0.0,\n",
    "                \"avg_relevance\": 0.0,\n",
    "                \"avg_completeness\": 0.0,\n",
    "                \"avg_usefulness\": 0.0,\n",
    "                \"avg_tool_appropriateness\": 0.0,\n",
    "                \"avg_tool_completeness\": 0.0,\n",
    "                \"avg_overall_response_score\": 0.0,\n",
    "                \"avg_overall_tool_score\": 0.0,\n",
    "                \"num_failed\": 0,\n",
    "            },\n",
    "            \"per_test\": [],\n",
    "            \"strengths\": [],\n",
    "            \"weaknesses\": [],\n",
    "            \"recommendations\": [],\n",
    "        }\n",
    "\n",
    "    total_acc = total_rel = total_comp = total_use = 0.0\n",
    "    total_resp_overall = 0.0\n",
    "    total_tool_approp = total_tool_comp = 0.0\n",
    "    total_tool_overall = 0.0\n",
    "\n",
    "    num_tests = len(test_results)\n",
    "    num_failed = 0\n",
    "\n",
    "    per_test_details = []\n",
    "\n",
    "    for tr in test_results:\n",
    "        question = tr.get(\"question\", \"\")\n",
    "        expected_resp = tr.get(\"expected_response\", \"\")\n",
    "        raw_resp = tr.get(\"response\", \"\")\n",
    "        messages = tr.get(\"messages\", [])\n",
    "        failed = tr.get(\"failed\", False)\n",
    "\n",
    "        # If failed, treat as very low-scoring with clear feedback\n",
    "        if failed:\n",
    "            num_failed += 1\n",
    "            resp_eval = {\n",
    "                \"accuracy\": {\"score\": 0.0, \"feedback\": \"Agent failed to produce a valid response.\"},\n",
    "                \"relevance\": {\"score\": 0.0, \"feedback\": \"No relevant response due to failure.\"},\n",
    "                \"completeness\": {\"score\": 0.0, \"feedback\": \"Response is incomplete or missing.\"},\n",
    "                \"usefulness\": {\"score\": 0.0, \"feedback\": \"No actionable guidance due to failure.\"},\n",
    "                \"overall_score\": 0.0,\n",
    "                \"overall_feedback\": \"Agent execution failed.\",\n",
    "            }\n",
    "            tool_eval = {\n",
    "                \"used_tools\": [],\n",
    "                \"expected_tools\": tr.get(\"expected_tools\", []),\n",
    "                \"tool_appropriateness\": {\"score\": 0.0, \"feedback\": \"No tools were used due to failure.\"},\n",
    "                \"tool_completeness\": {\"score\": 0.0, \"feedback\": \"Expected tools were not invoked.\"},\n",
    "                \"overall_score\": 0.0,\n",
    "                \"overall_feedback\": \"Tool execution did not occur.\",\n",
    "            }\n",
    "        else:\n",
    "            # Evaluate response\n",
    "            resp_eval = evaluate_response(\n",
    "                question=question,\n",
    "                final_response=str(raw_resp),\n",
    "                expected_response=expected_resp,\n",
    "            )\n",
    "\n",
    "            # Evaluate tool usage\n",
    "            tool_eval = evaluate_tool_usage(\n",
    "                messages=messages,\n",
    "                expected_tools=tr.get(\"expected_tools\", []),\n",
    "            )\n",
    "\n",
    "        # Aggregate scores\n",
    "        total_acc += resp_eval[\"accuracy\"][\"score\"]\n",
    "        total_rel += resp_eval[\"relevance\"][\"score\"]\n",
    "        total_comp += resp_eval[\"completeness\"][\"score\"]\n",
    "        total_use += resp_eval[\"usefulness\"][\"score\"]\n",
    "        total_resp_overall += resp_eval[\"overall_score\"]\n",
    "\n",
    "        total_tool_approp += tool_eval[\"tool_appropriateness\"][\"score\"]\n",
    "        total_tool_comp += tool_eval[\"tool_completeness\"][\"score\"]\n",
    "        total_tool_overall += tool_eval[\"overall_score\"]\n",
    "\n",
    "        per_test_details.append(\n",
    "            {\n",
    "                \"test_id\": tr.get(\"test_id\"),\n",
    "                \"question\": question,\n",
    "                \"response_snippet\": str(raw_resp)[:280],\n",
    "                \"response_metrics\": resp_eval,\n",
    "                \"tool_metrics\": tool_eval,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Compute averages\n",
    "    avg_accuracy = round(total_acc / num_tests, 2)\n",
    "    avg_relevance = round(total_rel / num_tests, 2)\n",
    "    avg_completeness = round(total_comp / num_tests, 2)\n",
    "    avg_usefulness = round(total_use / num_tests, 2)\n",
    "    avg_resp_overall = round(total_resp_overall / num_tests, 2)\n",
    "    avg_tool_approp = round(total_tool_approp / num_tests, 2)\n",
    "    avg_tool_comp = round(total_tool_comp / num_tests, 2)\n",
    "    avg_tool_overall = round(total_tool_overall / num_tests, 2)\n",
    "\n",
    "    # High-level strengths & weaknesses from averages\n",
    "    strengths = []\n",
    "    weaknesses = []\n",
    "    recommendations = []\n",
    "\n",
    "    if avg_relevance > 0.7:\n",
    "        strengths.append(\"Responses are generally well-aligned with the user questions.\")\n",
    "    else:\n",
    "        weaknesses.append(\"Relevance is moderate/low; some answers drift off-topic.\")\n",
    "        recommendations.append(\n",
    "            \"Tighten the instructions so the agent always directly answers the core question first.\"\n",
    "        )\n",
    "\n",
    "    if avg_completeness > 0.7:\n",
    "        strengths.append(\"Most responses cover the key aspects expected for the scenarios.\")\n",
    "    else:\n",
    "        weaknesses.append(\"Completeness is lacking; some responses miss time, cost, or solar details.\")\n",
    "        recommendations.append(\n",
    "            \"Emphasize in the system prompt that answers must include time windows, pricing, and solar considerations when applicable.\"\n",
    "        )\n",
    "\n",
    "    if avg_tool_approp > 0.7:\n",
    "        strengths.append(\"The agent usually selects appropriate tools for each task.\")\n",
    "    else:\n",
    "        weaknesses.append(\"Tool selection is sometimes suboptimal or incomplete.\")\n",
    "        recommendations.append(\n",
    "            \"Refine tool call examples in the system prompt so the agent knows exactly which tools to use for EV charging, thermostat, tips, and history-based questions.\"\n",
    "        )\n",
    "\n",
    "    if avg_tool_comp < 0.7:\n",
    "        weaknesses.append(\"Not all expected tools are being used in multi-tool scenarios.\")\n",
    "        recommendations.append(\n",
    "            \"Encourage explicit combination of weather + pricing + history tools for optimization questions.\"\n",
    "        )\n",
    "\n",
    "    if num_failed > 0:\n",
    "        weaknesses.append(f\"{num_failed} test(s) resulted in internal errors.\")\n",
    "        recommendations.append(\n",
    "            \"Harden the agent against tool errors and validation issues; catch exceptions in tools and return structured error messages the agent can interpret.\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"num_tests\": num_tests,\n",
    "            \"num_failed\": num_failed,\n",
    "            \"avg_accuracy\": avg_accuracy,\n",
    "            \"avg_relevance\": avg_relevance,\n",
    "            \"avg_completeness\": avg_completeness,\n",
    "            \"avg_usefulness\": avg_usefulness,\n",
    "            \"avg_overall_response_score\": avg_resp_overall,\n",
    "            \"avg_tool_appropriateness\": avg_tool_approp,\n",
    "            \"avg_tool_completeness\": avg_tool_comp,\n",
    "            \"avg_overall_tool_score\": avg_tool_overall,\n",
    "        },\n",
    "        \"per_test\": per_test_details,\n",
    "        \"strengths\": strengths,\n",
    "        \"weaknesses\": weaknesses,\n",
    "        \"recommendations\": recommendations,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b80d691-145f-46fe-9432-ccf03e6cc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_report(report: Dict[str, Any]) -> None:\n",
    "    \"\"\"Pretty print the evaluation report.\"\"\"\n",
    "    summary = report.get(\"summary\", {})\n",
    "    per_test = report.get(\"per_test\", [])\n",
    "    strengths = report.get(\"strengths\", [])\n",
    "    weaknesses = report.get(\"weaknesses\", [])\n",
    "    recommendations = report.get(\"recommendations\", [])\n",
    "\n",
    "    print(\"====================================================\")\n",
    "    print(\"      EcoHome Energy Advisor - Evaluation Report    \")\n",
    "    print(\"====================================================\\n\")\n",
    "\n",
    "    print(\"=== Summary Metrics ===\")\n",
    "    print(f\"Total tests        : {summary.get('num_tests', 0)}\")\n",
    "    print(f\"Failed tests       : {summary.get('num_failed', 0)}\")\n",
    "    print(f\"Avg Accuracy       : {summary.get('avg_accuracy', 0.0):.2f}\")\n",
    "    print(f\"Avg Relevance      : {summary.get('avg_relevance', 0.0):.2f}\")\n",
    "    print(f\"Avg Completeness   : {summary.get('avg_completeness', 0.0):.2f}\")\n",
    "    print(f\"Avg Usefulness     : {summary.get('avg_usefulness', 0.0):.2f}\")\n",
    "    print(f\"Avg Resp. Overall  : {summary.get('avg_overall_response_score', 0.0):.2f}\")\n",
    "    print(f\"Avg Tool Appropri. : {summary.get('avg_tool_appropriateness', 0.0):.2f}\")\n",
    "    print(f\"Avg Tool Completeness: {summary.get('avg_tool_completeness', 0.0):.2f}\")\n",
    "    print(f\"Avg Tool Overall   : {summary.get('avg_overall_tool_score', 0.0):.2f}\")\n",
    "    print()\n",
    "\n",
    "    print(\"=== Per-Test Details  ===\")\n",
    "    for t in per_test:\n",
    "        print(f\"- Test ID  : {t['test_id']}\")\n",
    "        print(f\"  Question : {t['question']}\")\n",
    "        print(f\"  Response : {t['response_snippet']!r}\")\n",
    "        rm = t[\"response_metrics\"]\n",
    "        tm = t[\"tool_metrics\"]\n",
    "        print(\n",
    "            f\"  Response Scores -> acc={rm['accuracy']['score']:.2f}, \"\n",
    "            f\"rel={rm['relevance']['score']:.2f}, \"\n",
    "            f\"comp={rm['completeness']['score']:.2f}, \"\n",
    "            f\"use={rm['usefulness']['score']:.2f}, \"\n",
    "            f\"overall={rm['overall_score']:.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Tool Scores     -> appr={tm['tool_appropriateness']['score']:.2f}, \"\n",
    "            f\"comp={tm['tool_completeness']['score']:.2f}, \"\n",
    "            f\"overall={tm['overall_score']:.2f}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    print(\"=== Strengths ===\")\n",
    "    if strengths:\n",
    "        for s in strengths:\n",
    "            print(f\"- {s}\")\n",
    "    else:\n",
    "        print(\"- (None identified yet)\")\n",
    "    print()\n",
    "\n",
    "    print(\"=== Weaknesses ===\")\n",
    "    if weaknesses:\n",
    "        for w in weaknesses:\n",
    "            print(f\"- {w}\")\n",
    "    else:\n",
    "        print(\"- (None identified yet)\")\n",
    "    print()\n",
    "\n",
    "    print(\"=== Recommendations ===\")\n",
    "    if recommendations:\n",
    "        for r in recommendations:\n",
    "            print(f\"- {r}\")\n",
    "    else:\n",
    "        print(\"- (No specific recommendations yet)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0752791-a990-4128-a6fe-80dd8ad50995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "      EcoHome Energy Advisor - Evaluation Report    \n",
      "====================================================\n",
      "\n",
      "=== Summary Metrics ===\n",
      "Total tests        : 10\n",
      "Failed tests       : 0\n",
      "Avg Accuracy       : 0.01\n",
      "Avg Relevance      : 0.03\n",
      "Avg Completeness   : 0.01\n",
      "Avg Usefulness     : 0.12\n",
      "Avg Resp. Overall  : 0.04\n",
      "Avg Tool Appropri. : 0.85\n",
      "Avg Tool Completeness: 0.90\n",
      "Avg Tool Overall   : 0.88\n",
      "\n",
      "=== Per-Test Details  ===\n",
      "- Test ID  : ev_charging_1\n",
      "  Question : When should I charge my electric car tomorrow to minimize cost and maximize solar power?\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='41b07427-a1d9-4b1e-af56-3ef8d6627158'), HumanMessage(content='When should I charge my electric car tomorrow to minimize \"\n",
      "  Response Scores -> acc=0.01, rel=0.04, comp=0.02, use=0.12, overall=0.05\n",
      "  Tool Scores     -> appr=1.00, comp=1.00, overall=1.00\n",
      "\n",
      "- Test ID  : thermostat_2\n",
      "  Question : What temperature should I set my thermostat on Wednesday afternoon to save money?\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='c29c46a3-b604-43e8-b76b-22edfdfd2449'), HumanMessage(content='When should I charge my electric car tomorrow to minimize \"\n",
      "  Response Scores -> acc=0.02, rel=0.04, comp=0.02, use=0.13, overall=0.05\n",
      "  Tool Scores     -> appr=1.00, comp=0.50, overall=0.75\n",
      "\n",
      "- Test ID  : dishwasher_3\n",
      "  Question : How much can I save by running my dishwasher at night instead of 6 PM?\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='af4239e1-f592-4144-acc4-79a8ef898b39'), HumanMessage(content='When should I charge my electric car tomorrow to minimize \"\n",
      "  Response Scores -> acc=0.02, rel=0.04, comp=0.02, use=0.13, overall=0.05\n",
      "  Tool Scores     -> appr=0.50, comp=0.50, overall=0.50\n",
      "\n",
      "- Test ID  : laundry_4\n",
      "  Question : When is the best time to run my washing machine this weekend?\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='80afb25a-8aca-488e-bd9d-1dab68672986'), HumanMessage(content='When should I charge my electric car tomorrow to minimize \"\n",
      "  Response Scores -> acc=0.01, rel=0.02, comp=0.01, use=0.11, overall=0.04\n",
      "  Tool Scores     -> appr=0.50, comp=1.00, overall=0.75\n",
      "\n",
      "- Test ID  : solar_forecast_5\n",
      "  Question : How much solar energy can I expect tomorrow in San Francisco?\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='37cd1522-c5ff-4640-9906-7cf9dda0fe8a'), HumanMessage(content='What temperature should I set my thermostat on Wednesday a\"\n",
      "  Response Scores -> acc=0.01, rel=0.03, comp=0.02, use=0.12, overall=0.04\n",
      "  Tool Scores     -> appr=1.00, comp=1.00, overall=1.00\n",
      "\n",
      "- Test ID  : usage_history_6\n",
      "  Question : Which appliance used the most electricity last month?\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='052c14b7-dc97-44cf-9791-ec0b1950f432'), HumanMessage(content='How much can I save by running my dishwasher at night inst\"\n",
      "  Response Scores -> acc=0.01, rel=0.02, comp=0.02, use=0.12, overall=0.04\n",
      "  Tool Scores     -> appr=1.00, comp=1.00, overall=1.00\n",
      "\n",
      "- Test ID  : optimization_multi_device_7\n",
      "  Question : Help me schedule my EV, dishwasher, and dryer tomorrow for lowest electricity cost.\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='9183e371-26b5-4220-803d-baea508cc04e'), HumanMessage(content='When is the best time to run my washing machine this weeke\"\n",
      "  Response Scores -> acc=0.01, rel=0.03, comp=0.01, use=0.12, overall=0.04\n",
      "  Tool Scores     -> appr=1.00, comp=1.00, overall=1.00\n",
      "\n",
      "- Test ID  : energy_tips_8\n",
      "  Question : Give me three ways to reduce electricity usage at home.\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='f4a56f31-39aa-423c-9938-5dccf16df0c0'), HumanMessage(content='How much solar energy can I expect tomorrow in San Francis\"\n",
      "  Response Scores -> acc=0.00, rel=0.02, comp=0.00, use=0.11, overall=0.03\n",
      "  Tool Scores     -> appr=0.50, comp=1.00, overall=0.75\n",
      "\n",
      "- Test ID  : recent_summary_9\n",
      "  Question : Summarize my energy usage over the past 48 hours.\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='f37e03b3-4d60-4699-8868-3ec1aab5fb90'), HumanMessage(content='Which appliance used the most electricity last month?', ad\"\n",
      "  Response Scores -> acc=0.01, rel=0.02, comp=0.02, use=0.12, overall=0.04\n",
      "  Tool Scores     -> appr=1.00, comp=1.00, overall=1.00\n",
      "\n",
      "- Test ID  : pool_pump_10\n",
      "  Question : What is the best time to run my pool pump this week?\n",
      "  Response : \"{'messages': [SystemMessage(content='Additional user context for personalization: Location: San Francisco, CA', additional_kwargs={}, response_metadata={}, id='7655bacd-cba8-4921-9c01-e3f20b7ae339'), HumanMessage(content='Help me schedule my EV, dishwasher, and dryer tomorrow for\"\n",
      "  Response Scores -> acc=0.01, rel=0.02, comp=0.01, use=0.11, overall=0.04\n",
      "  Tool Scores     -> appr=1.00, comp=1.00, overall=1.00\n",
      "\n",
      "=== Strengths ===\n",
      "- The agent usually selects appropriate tools for each task.\n",
      "\n",
      "=== Weaknesses ===\n",
      "- Relevance is moderate/low; some answers drift off-topic.\n",
      "- Completeness is lacking; some responses miss time, cost, or solar details.\n",
      "\n",
      "=== Recommendations ===\n",
      "- Tighten the instructions so the agent always directly answers the core question first.\n",
      "- Emphasize in the system prompt that answers must include time windows, pricing, and solar considerations when applicable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = generate_evaluation_report(test_results)\n",
    "display_evaluation_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30335d3c-d120-4729-aa62-0e9ec06b08e5",
   "metadata": {},
   "source": [
    "## Summary of the Report\n",
    "\n",
    ". All 10 test cases executed successfully\n",
    "\n",
    ". No runtime or tool-execution failures\n",
    "\n",
    ". Agent is stable and responsive\n",
    "\n",
    "### Performance Insights\n",
    "\n",
    "***Strength — Tool Usage***\n",
    "\n",
    "Tool Appropriateness: 0.85 avg\n",
    "- The agent usually selects the right tools\n",
    "\n",
    "Tool Completeness: 0.90 avg\n",
    "- It often uses all tools needed for the task\n",
    "\n",
    "Tool Overall Score: 0.88\n",
    "- Strong reasoning about when to call tools\n",
    "\n",
    "This indicates good understanding of:\n",
    "Weather + pricing optimization\n",
    "Usage-history questions\n",
    "EV scheduling\n",
    "RAG-based energy tips\n",
    "\n",
    "### Weakness — Response Quality\n",
    "\n",
    "Average response scoring:\n",
    "\n",
    "| Metric        | Score |\n",
    "|--------------|-------|\n",
    "| Accuracy     | 0.01  |\n",
    "| Relevance    | 0.03  |\n",
    "| Completeness | 0.01  |\n",
    "| Usefulness   | 0.12  |\n",
    "| Overall      | 0.04  |\n",
    "\n",
    "This means:\n",
    "Answers don’t fully address the user question\n",
    "Missing key details (hours, prices, savings, solar data)\n",
    "Responses sometimes drift or remain generic\n",
    "Not integrating retrieved tool outputs into explanations\n",
    "\n",
    "### Recommendations for Improvement\n",
    "\n",
    "Improve post-tool reasoning requiring the agent to summarize tool results in sentences\n",
    "Example: “Solar peaks at 1–3 PM, rates lowest at midnight—so charge at ___”\n",
    "Update system prompt to explicitly include Specific time windows, Pricing numbers, Solar irradiance references\n",
    "Enforce structured response format\n",
    "such as  {best_time, rationale, cost_estimate, solar_factor}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae9b82-893c-432a-97d4-ea25f924917a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ecohome)",
   "language": "python",
   "name": "ecohome"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
